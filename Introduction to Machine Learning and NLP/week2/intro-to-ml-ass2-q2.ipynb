{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":742210,"sourceType":"datasetVersion","datasetId":17},{"sourceId":126534,"sourceType":"datasetVersion","datasetId":64427}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport nltk\nimport re\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom gensim.models import KeyedVectors\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\nnltk.download('punkt')\nnltk.download('wordnet')\nnltk.download('stopwords')\n\ndf = pd.read_csv(\"/kaggle/input/twitter-airline-sentiment/Tweets.csv\")[['airline_sentiment', 'text']]","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-24T18:15:12.254480Z","iopub.execute_input":"2025-06-24T18:15:12.254806Z","iopub.status.idle":"2025-06-24T18:16:07.578089Z","shell.execute_reply.started":"2025-06-24T18:15:12.254780Z","shell.execute_reply":"2025-06-24T18:16:07.576929Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"lemmatizer = WordNetLemmatizer()\nstop_words = set(stopwords.words('english'))\n\ndef clean_tweet(text):\n    text = text.lower()\n    text = re.sub(r\"http\\S+|@\\S+|#\\S+|[^a-z\\s]\", \"\", text)\n    tokens = word_tokenize(text)\n    tokens = [lemmatizer.lemmatize(w) for w in tokens if w.isalpha() and w not in stop_words]\n    return tokens\n\nw2v = KeyedVectors.load_word2vec_format(\"/kaggle/input/gnewsvector/GoogleNews-vectors-negative300.bin\", binary=True)\n\ndef vectorize_tweet(text):\n    words = clean_tweet(text)\n    vectors = [w2v[w] for w in words if w in w2v]\n    return np.mean(vectors, axis=0) if vectors else np.zeros(300)\n\nX = np.array([vectorize_tweet(t) for t in df['text']])\ny = df['airline_sentiment'].map({'negative': 0, 'neutral': 1, 'positive': 2}).values","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T18:16:07.579310Z","iopub.execute_input":"2025-06-24T18:16:07.579652Z","iopub.status.idle":"2025-06-24T18:17:23.816660Z","shell.execute_reply.started":"2025-06-24T18:16:07.579618Z","shell.execute_reply":"2025-06-24T18:17:23.815540Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nclf = LogisticRegression(max_iter=1000, multi_class='multinomial')\nclf.fit(X_train, y_train)\n\ny_pred = clf.predict(X_test)\nprint(\"Test Accuracy:\", accuracy_score(y_test, y_pred))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T18:17:23.818662Z","iopub.execute_input":"2025-06-24T18:17:23.818979Z","iopub.status.idle":"2025-06-24T18:17:27.417946Z","shell.execute_reply.started":"2025-06-24T18:17:23.818954Z","shell.execute_reply":"2025-06-24T18:17:27.417248Z"}},"outputs":[{"name":"stdout","text":"Test Accuracy: 0.7783469945355191\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"def predict_tweet_sentiment(model, w2v_model, tweet):\n    words = clean_tweet(tweet)\n    vectors = [w2v_model[w] for w in words if w in w2v_model]\n    vec = np.mean(vectors, axis=0) if vectors else np.zeros(300)\n    label = model.predict(vec.reshape(1, -1))[0]\n    return {0: 'negative', 1: 'neutral', 2: 'positive'}[label]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T18:17:27.418608Z","iopub.execute_input":"2025-06-24T18:17:27.418860Z","iopub.status.idle":"2025-06-24T18:17:27.428615Z","shell.execute_reply.started":"2025-06-24T18:17:27.418836Z","shell.execute_reply":"2025-06-24T18:17:27.425303Z"}},"outputs":[],"execution_count":4}]}